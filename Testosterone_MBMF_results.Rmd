---
title: "Testosterone effects on model-based vs model-free learning"
author: "NM"
date: "12 2 2021"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Background

Decision making can to a degree be described as being either intuitive and "quick and dirty" or deliberate and thought-through. Although this dichotomy has its limitations, it has provided grounds for fruitful discussion across several psychological disciplines. There has been some debate whether the hormone testosterone biases decision making towards more intuitive. In this study we administered Testosterone in a between subject fashion and employed a sequential decision making task that is designed to probe at the trade-off between intuitive/reflexive and deliberate/prospective decision making styles. We analyzed subjects behavior with a computational model, that describes subjects' trial by trial choices in terms of a trade-off between a habitual or model-free and a goal-directed or model-based component. A model-free agent learns only through direct experience via reinforcement: it is more likely to repeat an action if it led to rewarding and less likely to repeat it if it led to negative outcomes in the past. Conversely, a model-based agent uses the causal model of the world to make choices based on the goal it wants to achieve. If testosterone promotes intuitive decision-making then we would expect a stronger effect of model-based values on choice selection in subjects under testosterone.




Notes:

likely no effects on model-based learning. but reduced model-free learning. 

- T group less likely to stay with winning and avoid losing in same state trials 
- T earn less points in same but not different trials

- when difficult, the groups appear to be similar

Modelling: trying frank softmax version for both kool and our model.

Belief model, didnt show much, altoughg there is something in behavior if i include interaction - not strong/important/ hard to interpret what it means. 



Genetics, something with dat1
once models done:

- include the stickiness factor (not necessary for now)
- include the genetics

- check with serum levels.. 

- loo models for comparison of kool and the w, also 
    running now.. 
- parameter retrieval for winning model?
    yes.

Frank admin model shows no effects on model based but negative effects of T on model free learning

Just reran the frank_pos model, that assumes the weights are positive (and not unconstrained)

  shows reduced model-free, but not model-based learning (or at least not intierly)

Also reran the same model with genetics
  waiting
  
  Kool model weird (beta_mb zero with little variance), trying now with 1 lr
  Same problem.. 
  preliminary model comparison favours our "nolr" model.
  
Genetics:
  based on stick slab, we included only variables with mean > 0.01, this yielded all three genxT_mb interactions dat1_mb and comt_g
  running again with only this four, keeping the shrinkage priors.

```{r load and prepare data, include = FALSE, eval = TRUE, message = FALSE}
# load packages -----------------------------------------------------------


library(dplyr)
library(smooth)
library(ggplot2)
library(reshape2)
library(hrbrthemes)
library(tidyr)
library(viridis)
library(gridExtra)
library(cowplot)
library(ggthemes)
library(knitr)

library(nlme)
library(brms)
# library(MASS)
library(tidyverse)  # ggplot, dplyr, and friends
library(ggridges)   # Ridge plots
library(ggstance)   # Horizontal pointranges and bars
library(patchwork)  # Lay out multiple ggplot plots; install from https://github.com/thomasp85/patchwork
library(scales)     # Nicer formatting for numbers
library(broom)
library(loo)
library(rstanarm)
library(rstan)
source("plotting_functions.r")
source("theme_functions_MB.r")
# load data ---------------------------------------------------------------

# call functions
source("plotting_functions.r")
source("theme_functions_MB.r")
logit <- function(x) log(x/(1-x))
inv_logit <- function(x) 1/(1+exp(-x))

# rm(list=ls())  # clear all


data_beh <- readRDS(file = "Group Data/data_beh.rds")
data_group <-readRDS(file = "Group Data/data_group.rds")

### prepare data
subjList <- unique(data_group$ID)




```

## Results

# The task design
The Two-step task is a sequential decision making task where subjects need to earn points in a dynamically changing environment. Subjects first see one of two possible starting scenarios, each featuring a pair of spaceship that fly deterministically to one of the two planets, where they encounter an alien that gives them points ranging from -4 to +5. How many points each alien gives changes throughout the experiment according to a random gaussian walk. The deterministic transition between the spaceships and the planets stays the same throughout the experiment. 

Subjects are explicitly instructed about the mapping from the spaceships to the planets.

!['TwostepTask'](Figures/Task_design1.png)
**Figure 1| Two step task **. Subjects  

# Testosterone affects subjects performance in trials where first states are the same as in the previous trial, but not when they are different

The higher the reward in the previous trial the more likely the subjects should be to stay with their previous choice of planets. However, when the first state of the trial is different to that of the previous trial, staying with the same planet requires that subjects keep the mapping from spaceships to planets online. We first analyzed the degree to which previous points affect the choice to stay with the spaceship that flies to the same planet. Specifically, we were interested in how this relationship changes with respect to previous states being the same or different, and under testosterone. Using a Bayesian multilevel logistic regression model we show that subjects under placebo are 1.60 times (95% CI [1.514, 1.667]) more likely to stay with the previous choice for each extra previous point (mean (95% CI) in logodds is 0.466 [0.414, 0.511]). As expected, the slope between previous points and staying behavior was reduced when first states of two consecutive trials were different (mean (95% CI) in logodds is -0.254 [-0.284, -0.228]), meaning subjects under placebo were only 1.237 times (95% CI[1.176, 1.288]) more likely to stay with the previous choice for each additional previous point. Interestingly, we find that under testosterone the effect of previous points on staying behavior is reduced when first states are the same (mean (95% CI) in logodds is -0.079, [-0.150, -0.016]) but not when first states are different (mean (95% CI) in logodds is -0.018 [-0.084, 0.041]). This suggests, that subjects under testosterone perform worse than subject under placebo in "easier" trials, but perform equally well in trials that require keeping the task structure online.

```{r staying behavior,fig.width=13, echo=FALSE, message= FALSE}


fit_model_beh <- readRDS("Brms_stay_beh.rds")
# fit_model_beh %>% summary()
post <- fit_model_beh %>% posterior_samples()
post$b_prev_points %>% exp() %>%  quantile(probs = c(0.5,0.025,0.95)) %>% round(3)
post$b_prev_points %>%  quantile(probs = c(0.5,0.025,0.95)) %>% round(3)
(post$b_prev_points + post$`b_prev_points:prev_state_diffdifferent`)  %>% exp() %>%  quantile(probs = c(0.5,0.025,0.95)) %>% round(3)

(post$b_prev_points + post$`b_adminTestosterone:prev_points`)  %>% exp() %>%  quantile(probs = c(0.5,0.025,0.95)) %>% round(3)

post$`b_prev_points:prev_state_diffdifferent` %>%  quantile(probs = c(0.5,0.025,0.95)) %>% round(3)
(post$`b_adminTestosterone:prev_points:prev_state_diffdifferent` + post$`b_adminTestosterone:prev_points`)  %>% quantile(probs = c(0.5,0.025,0.95)) %>% round(3)
(post$`b_adminTestosterone:prev_points`)  %>% quantile(probs = c(0.5,0.025,0.95)) %>% round(3)
post$`b_adminTestosterone:prev_points`
pars_set = c("b_prev_points",
            "b_prev_state_diffdifferent",
            "b_adminTestosterone:prev_points",
            "b_adminTestosterone:prev_state_diffdifferent" 
            )
# pars_all <- rev(pars_all)
# pars_all_label <- rev(pars_all_label)
# pars_all_reduced <- rev(pars_all_reduced)
# pars_all_reduced_label <- rev(pars_all_reduced_label)
# mcmc_plot(brms_investment_genotype_logtrials)
# investment_stat_plot <- mcmc_plot(brms_investment_genotype_logtrials, pars = c("b_"), prob = 0.80, prob_outer = 0.95) + geom_vline(xintercept=0, size=0.5, alpha=0.5, linetype=2) + theme_Publication() +
#   theme(axis.line = element_line(colour = "black"))   +  scale_y_discrete(limits = pars_all , labels =pars_all_label)



g_stats  <- mcmc_plot(fit_model_beh, pars = pars_set)
# fit_model_beh <- readRDS("Brms_stay_beh_winloss.rds")
# 
# fit_model_beh <- readRDS("Brms_stay_beh_belief.rds")


post[,1:6] %>% glimpse()
post <- posterior_samples(fit_model_beh)
post_fu <- function(x, funct_var) {
 
  admin =  x[2]
  prev_points = x[1]
  prev_state =  x[3]
  fitted = post$b_Intercept +  post$b_prev_points * prev_points +  post$b_prev_state_diffdifferent*prev_state+  post$`b_prev_points:prev_state_diffdifferent`* prev_points*prev_state +  post$`b_adminTestosterone`*admin +  post$`b_adminTestosterone:prev_points`* prev_points*admin +  post$`b_adminTestosterone:prev_state_diffdifferent`*prev_state*admin +  post$`b_adminTestosterone:prev_points:prev_state_diffdifferent`* prev_points*prev_state*admin
  
  
   # y_lo <- funct_var(fitted)
   y <- funct_var(inv_logit(fitted))
   return(y)
}

nd <- expand_grid(prev_points = c(-4:5), admin = c(0,1), prev_state = c(0,1))

nd_mat <- nd %>% as.matrix()

nd_mean = apply(nd_mat,1, post_fu, funct_var = mean)
nd_lCI = apply(nd_mat,1, post_fu, funct_var = function(x) quantile(x, probs = 0.05))
nd_uCI = apply(nd_mat,1, post_fu, funct_var = function(x) quantile(x, probs = 0.95))
nd_sd = apply(nd_mat,1, post_fu, funct_var = sd)
nd <- nd %>% mutate(mean_value  = nd_mean,
                    low_CI  = nd_lCI,
                    up_CI  = nd_uCI,
                    sd_value  = nd_sd,
                    admin = factor(admin, levels= c(0,1), labels = c("Placebo", "Testosterone")))

# g1 <- nd %>% filter(prev_state == 1) %>% ggplot(aes(x= prev_points, y = mean_value, fill = admin)) + geom_ribbon(aes(ymin = low_CI, ymax = up_CI), alpha = 0.5) + geom_line(aes(colour = admin)) + theme_Publication() 
# g2 <- nd %>% filter(prev_state == 0) %>% ggplot(aes(x= prev_points, y = mean_value, fill = admin)) + geom_ribbon(aes(ymin = low_CI, ymax = up_CI), alpha = 0.5) + geom_line(aes(colour = admin)) + theme_Publication() + theme(legend.position = none)
data_same <- data_beh %>% filter(!is.na(stay), prev_state_diff == "same") %>% group_by(admin, prev_points) %>% summarize(N = n(), 
                                                                                                              mean_stay = mean(stay),
                                                                                                              se_stay = sd(stay)/sqrt(N))
data_diff<- data_beh %>% filter(!is.na(stay), prev_state_diff != "same") %>% group_by(admin, prev_points) %>% summarize(N = n(), 
                                                                                                              mean_stay = mean(stay),
                                                                                                              se_stay = sd(stay)/sqrt(N))
  
    
    # data_beh$admin %>% unique()
g_different <- nd %>% filter(prev_state == 1) %>% ggplot(aes(x= prev_points, y = mean_value, fill = admin)) + 
  geom_errorbar(data=data_diff, aes(x = prev_points, y = mean_stay,ymin = mean_stay - se_stay, ymax =mean_stay + se_stay), width = 0, position = position_dodge(0.2))+
  geom_point(data=data_diff, aes(x = prev_points, y = mean_stay, colour = admin),position = position_dodge(0.2)) +
  geom_ribbon(aes(ymin = mean_value-sd_value, ymax = mean_value+sd_value), alpha = 0.5) + 
  geom_line(aes(colour = admin)) + theme_Publication()  + ylim(c(0.2,1)) + 
  theme(legend.position = c(0.7,0.2), legend.title = element_blank(),  axis.text.x = element_blank(), axis.ticks = element_blank())+ 
  xlab("Previous Points")+ ylab("P(stay) after \ndifferent first state")  
  
g_same <- nd %>% filter(prev_state == 0) %>% ggplot(aes(x= prev_points, y = mean_value, fill = admin)) + 
  geom_errorbar(data=data_same, aes(x = prev_points, y = mean_stay,ymin = mean_stay - se_stay, ymax =mean_stay + se_stay), width = 0, position = position_dodge(0.2))+
  geom_point(data=data_same, aes(x = prev_points, y = mean_stay, colour = admin),position = position_dodge(0.2)) +
  geom_ribbon(aes(ymin = mean_value-sd_value, ymax = mean_value+sd_value), alpha = 0.5) + 
  geom_line(aes(colour = admin)) + theme_Publication() + ylim(c(0.2,1)) + 
  theme(legend.position = "none",  axis.text.x = element_blank(),axis.ticks = element_blank())+ 
  xlab("Previous Points")+  ylab("P(stay) after \nsame first state")
 
 # g_legend <- ggplot(nd,aes(x= prev_points, y = mean_value, fill = admin)) +  geom_line(aes(colour = admin)) + theme(legend.position = "bottom")
# g_legend<-  get_legend(g_legend)
# (g_same / g_different)
plot_grid(
plot_grid(g_same, g_different, nrow =1),
g_stats,
ncol = 1,
rel_heights = c(1,0.5))
#+ geom_line()

```
Subjects under testosterone also earned less points when the first state was the same as in the previous trials (mean difference in points is -0.096, 95% CI[-0.191, -0.001], but not when the first state was different (mean difference in points is -0.022, 95% CI[-0.097, 0.054]).

```{r staying behavior,fig.width=12, echo=FALSE, message= FALSE}


fit_model_earn <- readRDS("Brms_points.rds")
# fit_model_earn %>% summary()
# post <- posterior_samples(fit_model_earn)

# (post$`b_adminTestosterone:prev_state_diffdifferent` + post$b_adminTestosterone) %>% quantile(probs = c(0.5,0.025, 0.975)) %>% round(3)
# post$b_adminTestosterone %>% quantile(probs = c(0.5,0.025, 0.975)) %>% round(3)



data_earn <- data_beh %>%  filter(!is.na(prev_state_diff)) %>%
  group_by(prev_state_diff, ID) %>% 
  summarize(N=n(), earn = mean(points),
            admin=admin[1])
data_box_earn <- data_earn %>% group_by(admin, prev_state_diff) %>% 
  summarize(N = n(),
            mean_earn = mean(earn), 
            se = sd(earn, na.rm = T)/sqrt(N))

g_earn <- ggplot(data_box_earn, aes(x = admin, y = mean_earn)) +
  geom_errorbar(aes(ymin = mean_earn - se, ymax = mean_earn +se), width=0.2, position = position_dodge(0.9)) +
  geom_point(aes(x = admin , y= mean_earn, colour = admin), position = position_dodge(width = 0.1), stat = "identity",  size = 3) +
  # geom_jitter(data  =data_earn, aes(x=admin, y=earn), size = 0.2) + 
  facet_wrap(~prev_state_diff) + theme_Publication() + theme(legend.position = "bottom", legend.title = element_blank()) + xlab("") +ylab("Points Earned") +scale_x_discrete(labels = c("P", "T" ))
g_stat_earn <- mcmc_plot(fit_model_earn, pars = "^b_")
g_earn * g_stat_earn
# 

```


# Computational modelling confirms that testosterone decreases the weight on model-free but not on model-based learning

To better estimate the trial by trial choices of subjects and to what degree they were driven by deliberate or intuitive processes we applied a computational model that estimates the degree to which model-based and model-free processes influence decisions. We designed a simple model whereby the model-based agent is always aware of the mapping between spaceships and planets, and therefore sets the subjective value of each of the spaceships as the value of the last outcome related to the planet if flies to. The model-free agent on the other hand simply remember the last outcome for each spaceship, whereby the subjective values of spaceships where the outcome is not encountered on each trial are discounted by subjects specific memory parameter g. The model then estimates the degree to which model-based and model-free differences in subjective values contribute to the choice on each trial. Importantly, we estimate the parameters in a full bayesian framework, thereby modelling both subject specific parameters and group parameters in one inferential step. The effect of administration is defined as a group level regression coefficient for all three parameters. 





```{r computational model results: plot best investment model,fig.width = 12, fig.height = 7, echo=FALSE, warning= FALSE}
if (!exists("fit_model")) {
fit_model <- readRDS("Model_results/M_frank_admin_pos.rds")
}
pars_all <- grep("^beta", names(fit_model), value = T)  
 # pairs(fit_model, pars = pars_all)
 # pairs(fit_model, pars = c('mu_p', 'sigma'))
g_m <- stan_plot(fit_model, pars = pars_all, outer_level = 0.95) + theme_Publication()+
  geom_vline(xintercept=0) +  labs(subtitle = "80% and 95% CI")+
  scale_y_discrete(name = "", limits = c("Testosterone on g","Testosterone on MF","Testosterone on MB")) +
  scale_x_continuous(name = "")



data_group$w_mb <- get_posterior_mean(fit_model, pars=c('w_mb'))[,5]
data_group$w_mf <- get_posterior_mean(fit_model, pars=c('w_mf'))[,5]
data_group$g <- get_posterior_mean(fit_model, pars=c('g'))[,5]
# data_sum = data_group %>% group_by(admin)%>% summarise(N=n(),
#                                                        mean_w_mb = mean(w_mb),
#                                                        se_w_mb = sd(w_mb)/sqrt(N),
#                                                        mean_w_mf = mean(w_mf),
#                                                        se_w_mf = sd(w_mf)/sqrt(N),
#                                                        mean_g = mean(g),
#                                                        se_g = sd(g)/sqrt(N))

g_w_mb <- ggplot(data = data_group, aes(x = admin, y = w_mb))+  # group = ID, linetype = Genotype)) 
  # geom_violin(aes(fill = admin))+
  geom_boxplot(aes(fill = admin), width=0.5, color="black", outlier.shape = NA)+
  # stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red") + 
  geom_jitter(size = 0.1) +  
  # geom_errorbar(data=data_sum, aes(x= admin, y = mean_w_mb, ymin = mean_w_mb - se_w_mb, ymax = mean_w_mb + se_w_mb), width = 0)+
  # geom_point(data=data_sum, aes(x=admin, y = mean_w_mb, colour = admin), size = 2) +
  # 
  theme_Publication() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major  = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank()) +
  ylab(expression("Weight on MB value"))+ ylim(c(0,1)) + # ylab(expression("\u03C9"[good])) +  
  xlab("") #+ discrete_scale("fill","Publication", manual_pal(values = c("#386cb0","#fdb462")))

# g_w_mb
g_legend <- get_legend(g_w_mb)
g_w_mf <- ggplot(data = data_group, aes(x = admin, y = w_mf))+  # group = ID, linetype = Genotype)) 
  # geom_violin(aes(fill = admin))+
  geom_boxplot(aes(fill = admin), width=0.5, color="black", outlier.shape = NA)+
  # stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red") + 
  geom_jitter(size = 0.1) +  theme_Publication() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major  = element_blank(),
        legend.position = "none") +
  ylab(expression("Weight on MF value")) + ylim(c(0,1))+ # ylab(expression("\u03C9"[good])) +  
  xlab("") #+ discrete_scale("fill","Publication", manual_pal(values = c("#386cb0","#fdb462")))

g_gamma <- ggplot(data = data_group, aes(x = admin, y = g))+  # group = ID, linetype = Genotype)) 
  # geom_violin(aes(fill = admin))+
  geom_boxplot(aes(fill = admin), width=0.5, color="black", outlier.shape = NA)+
  # stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red") + 
  geom_jitter(size = 0.1) +  theme_Publication() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major  = element_blank(),
        legend.position = "none") +
  ylab(expression("Discounting parameter")) + ylim(c(0,1))+# ylab(expression("\u03C9"[good])) +  
  xlab("") #+ discrete_scale("fill","Publication", manual_pal(values = c("#386cb0","#fdb462")))


plot_grid(
plot_grid(g_w_mb+ theme(legend.position = "none"),g_w_mf,g_gamma, ncol = 3),
g_legend, g_m, ncol = 1, rel_heights = c(1,0.05,0.7))

```
# Genetics 

We estimated the main effects of genotype and the interaction effects of genotype and testosterone administration on the computational parameters in two stes. We first fitted a hierarchical computational model including the effects of all genotype variables and the interactions of genotype and testosterone on all computational parameters. We used spike and slab shrinkage priors for regularization. We then selected effects where the means of posterior distributions were above 0.1 and reran the model estimates using only these regressors.

We find that all three genotypes interact with the effect of testosterone on model based behavior, but not with model-free. In particular testosterone seems to decrease model based behavior in subjects with either low prefrontal dopamine availability, indexed by the ValVal variant of the COMT polymorphisms, or high striatal dopamine levels as indexed the be carriers of at least one 9-repeat allele of the Dat1 polymorphism. Similarly, subjects with more CAG repeats are more likely to have their model-based weight increased by testosterone.

```{r genetics results:,fig.width = 9, fig.height = 10, echo=FALSE, warning= FALSE}
if (!exists("fit_model_gen")) {
  fit_model_gen <- readRDS("Model_results/M_gen_red.rds") }

pars_all <- grep("^beta", names(fit_model_gen), value = T)  
pars_all <- c("beta_comt_g", "beta_dat1_mb", "beta_comt_testosterone_mb", "beta_dat1_testosterone_mb", "beta_cag_testosterone_mb")
 

 # pairs(fit_model, pars = pars_all)
 # pairs(fit_model, pars = c('mu_p', 'sigma'))
g_m_gen <- stan_plot(fit_model_gen, pars = pars_all, outer_level = 0.95) + theme_Publication()+
  geom_vline(xintercept=0) + # labs(subtitle = "80% and 95% CI") +
  # scale_y_discrete(name = "") + limits = c("Testosterone on g","Testosterone on MF","Testosterone on MB")) +
  scale_x_continuous(name = element_blank())

# gen_par <-"comt_f"
# gen_par <- sym(gen_par)
# levels(data_temp_gen$comt) =  c("Low\nPFC DA\n(Val/Val)", "\n(Met/Val)", "High\nPFC DA\n(Met/Met)")

data_group_gen <- data_group
data_group_gen$w_mb <- get_posterior_mean(fit_model_gen, pars=c('w_mb'))[,5]
data_group_gen$w_mf <- get_posterior_mean(fit_model_gen, pars=c('w_mf'))[,5]
data_group_gen$g <- get_posterior_mean(fit_model_gen, pars=c('g'))[,5]



g_w_mb_comt <- data_group_gen %>% group_by(admin,comt_f)%>% summarise(N=n(),
                                                       mean_w_mb = mean(w_mb),
                                                       se_w_mb = sd(w_mb)/sqrt(N),
                                                       mean_w_mf = mean(w_mf),
                                                       se_w_mf = sd(w_mf)/sqrt(N),
                                                       mean_g = mean(g),
                                                       se_g = sd(g)/sqrt(N)) %>% 


ggplot(aes(x = admin, y = w_mb))+  # group = ID, linetype = Genotype)) 
  geom_errorbar(aes(x= admin, y = mean_w_mb, ymin = mean_w_mb - se_w_mb, ymax = mean_w_mb + se_w_mb), width = 0)+
  geom_point(aes(x=admin, y = mean_w_mb, colour = admin), size = 2) +
  # 
  # geom_violin(aes(fill = admin))+
  # geom_boxplot(aes(fill = admin), width=0.5, color="black")+
  # stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red") + 
  # geom_jitter(data = data_group_gen, size = 0.1) +  
  theme_Publication() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major  = element_blank(),
        legend.position = "right",
        legend.title = element_blank()) +
  ylab(expression("Weight on MB value"))+ ylim(c(0,0.3)) + 
  xlab("COMT polymorphism")  + facet_wrap(~comt_f) 

# g_w_mb_comt + theme(legend.position = "none")

data_group_gen$cag_f <- factor(data_group_gen$cag < median(data_group_gen$cag, na.rm = TRUE), levels = c(FALSE, TRUE), labels = c("high cag\nrepeats", "low cag\nrepeats"))

g_w_mb_cag <- data_group_gen %>% group_by(admin,cag_f)%>% summarise(N=n(),
                                                       mean_w_mb = mean(w_mb),
                                                       se_w_mb = sd(w_mb)/sqrt(N),
                                                       mean_w_mf = mean(w_mf),
                                                       se_w_mf = sd(w_mf)/sqrt(N),
                                                       mean_g = mean(g),
                                                       se_g = sd(g)/sqrt(N)) %>% 


ggplot(aes(x = admin, y = w_mb))+  # group = ID, linetype = Genotype)) 
  geom_errorbar(aes(x= admin, y = mean_w_mb, ymin = mean_w_mb - se_w_mb, ymax = mean_w_mb + se_w_mb), width = 0)+
  geom_point(aes(x=admin, y = mean_w_mb, colour = admin), size = 2) +
  # 
  # geom_violin(aes(fill = admin))+
  # geom_boxplot(aes(fill = admin), width=0.5, color="black")+
  # stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red") + 
  # geom_jitter(data = data_group_gen, size = 0.1) +  
  theme_Publication() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major  = element_blank(),
        legend.position = "none") +
  ylab(expression("Weight on MB value"))+ ylim(c(0,0.3)) + 
  xlab("CAG polymorphism")  + facet_wrap(~cag_f) 

# g_w_mb_cag + theme(legend.position = "none")

# data_group_gen$dat1 %>% glimpse()
g_w_mb_dat1 <- data_group_gen %>% group_by(admin,dat1)%>% summarise(N=n(),
                                                       mean_w_mb = mean(w_mb),
                                                       se_w_mb = sd(w_mb)/sqrt(N),
                                                       mean_w_mf = mean(w_mf),
                                                       se_w_mf = sd(w_mf)/sqrt(N),
                                                       mean_g = mean(g),
                                                       se_g = sd(g)/sqrt(N)) %>% 


ggplot(aes(x = admin, y = w_mb))+  # group = ID, linetype = Genotype)) 
  geom_errorbar(aes(x= admin, y = mean_w_mb, ymin = mean_w_mb - se_w_mb, ymax = mean_w_mb + se_w_mb), width = 0)+
  geom_point(aes(x=admin, y = mean_w_mb, colour = admin), size = 2) +
  # 
  # geom_violin(aes(fill = admin))+
  # geom_boxplot(aes(fill = admin), width=0.5, color="black")+
  # stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red") + 
  # geom_jitter(data = data_group_gen, size = 0.1) +  
  theme_Publication() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major  = element_blank(),
        legend.position = "none") +
  ylab(expression("Weight on MB value"))+ ylim(c(0,0.3)) + 
  xlab("Dat1 polymorphism")  + facet_wrap(~dat1) 

g_legend <- get_legend(g_w_mb_comt)
plot_grid(
plot_grid(g_w_mb_comt+ theme(legend.position = "none"), g_legend, nrow = 1, rel_widths = c(1,0.3)),
plot_grid(g_w_mb_dat1, g_w_mb_cag, nrow = 1),
g_m_gen,
rel_heights = c(1,1,0.7),
ncol = 1)


```



## Supplementary material

# Model comparison


```{r compare model stats, echo=FALSE, warning= FALSE, massage = FALSE}


#  stan model compare models  -------------------
# M_nolr_loo <- readRDS("~/mnt/p/userdata/mikusn22/data/Hana MBMF Testo/Model_results/M_nolr_loo.rds")
# M_kool_1lr_loo <- readRDS("~/mnt/p/userdata/mikusn22/data/Hana MBMF Testo/Model_results/M_kool_1lr_loo.rds")
# M_kool_2lr_loo <- readRDS("~/mnt/p/userdata/mikusn22/data/Hana MBMF Testo/Model_results/M_kool_2lr_loo.rds")
# loo.M_nolr_loo <- loo::loo(M_nolr_loo)
# loo.M_kool_1lr_loo<- loo::loo(M_kool_1lr_loo)
# loo.M_kool_2lr_loo<- loo::loo(M_kool_2lr_loo)
# 
# pbma_BB_wts <- pseudobma_weights(cbind(loo.M_nolr_loo$pointwise[,"elpd_loo"],
#                                        loo.M_kool_1lr_loo$pointwise[,"elpd_loo"],
#                                        loo.M_kool_2lr_loo$pointwise[,"elpd_loo"]))
# 
# cdata <- loo_compare(loo.M_nolr_loo, loo.M_kool_1lr_loo, loo.M_kool_2lr_loo)
# 
# cData <- as.data.frame(cdata)
# cData <- cData%>% mutate(model = c(1,3,2), BB_wts = pbma_BB_wts)
# saveRDS(cData, file ="Model_comparison.rds")

cData <- readRDS("Model_comparison.rds")
g_compare_models <- ggplot(cData, aes(x = model, y =  elpd_diff)) +  
 

geom_errorbar(aes(ymin= elpd_diff - se_diff, ymax = elpd_diff+se_diff), width = 0.2, position = position_dodge(0.9)) + theme_Publication() +
  geom_bar(position = position_dodge(), stat = "identity") +  scale_x_discrete(name="", limits = c("M1", "M2", "M3")) + 
  
    ylab("Comparing expected\nlog predictive density")

g_compare_models
  

```



#  parameter retrieval  

For each subject we selected 5 paramter sets drawn from that subjects mean and standard deviation and simulated data from those parameters. We then refitted the model from the simulated data and found that the parameters recover very well.
```{r, echo=FALSE, warning= FALSE, fig.width = 12, eval= FALSE}
# savedataset = "Refit_lkj_hgf_pw"  
# drf_temp <- readRDS(paste(savedataset, "_pars_all.rds", sep=""))
# drf_temp$om_mean <- 1/2*(drf_temp$om_good +drf_temp$om_bad)
# drf_temp$om_mean_rf <- 1/2*(drf_temp$om_good_rf +drf_temp$om_bad_rf)
# 
# drf_temp$om_diff <- drf_temp$om_good -drf_temp$om_bad
# # drf_temp$om_diff_rf <- drf_temp$om_good_rf -drf_temp$om_bad_rf
# # cor.test(drf_temp$om_diff,drf_temp$om_diff_rf)
# g_rf_om_mean<- ggplot(data =drf_temp, aes(x= om_mean, y = om_mean_rf)) + 
#   geom_point()+
#   geom_smooth(method = "lm") + 
#   # coord_cartesian(xlim = c(-8,0), ylim = c(-8,0)) +   
#   theme_Publication() + 
#   theme(axis.text.x = element_blank(),
#         axis.ticks.x = element_blank(),
#         panel.grid.major  = element_blank(),
#         legend.position = "none") +   ylab("refitted om") + xlab("om") + 
#   labs(title=paste("corr = ", round(cor.test(drf_temp$om_mean,drf_temp$om_mean_rf)[["estimate"]],2), sep=""))
# 
# # g_rf_om_good<- ggplot(data =drf_temp, aes(x= om_good, y = om_good_rf)) + 
# #   geom_point()+
# #   geom_smooth(method = "lm") + 
# #   # coord_cartesian(xlim = c(-8,0), ylim = c(-8,0)) +   
# #   theme_Publication() + 
# #   theme(axis.text.x = element_blank(),
# #         axis.ticks.x = element_blank(),
# #         panel.grid.major  = element_blank(),
# #         legend.position = "none") +   ylab("refitted om_good") + xlab("om_good") + 
# #   labs(title=paste("corr = ", round(cor.test(drf_temp$om_good,drf_temp$om_good_rf)[["estimate"]],2), sep=""))
# 
# # g_rf_om_bad<- ggplot(data =drf_temp, aes(x= om_bad, y = om_bad_rf)) + 
# #   geom_point()+
# #   geom_smooth(method = "lm") + 
# #   # coord_cartesian(xlim = c(-8,0), ylim = c(-8,0)) +   
# #   theme_Publication() + 
# #   theme(axis.text.x = element_blank(),
# #         axis.ticks.x = element_blank(),
# #         panel.grid.major  = element_blank(),
# #         legend.position = "none") +   ylab("refitted om_bad") + xlab("om_bad") + 
# #   labs(title=paste("corr = ", round(cor.test(drf_temp$om_bad,drf_temp$om_bad_rf)[["estimate"]],2), sep=""))
# 
# g_rf_noise<- ggplot(data =drf_temp, aes(x= noise, y = noise_rf)) + 
#   geom_point()+
#   geom_smooth(method = "lm") + 
#   # coord_cartesian(xlim = c(-8,0), ylim = c(-8,0)) +   
#   theme_Publication() + 
#   theme(axis.text.x = element_blank(),
#         axis.ticks.x = element_blank(),
#         panel.grid.major  = element_blank(),
#         legend.position = "none") +   ylab("refitted noise") + xlab("noise") + 
#   labs(title=paste("corr = ", round(cor.test(drf_temp$noise,drf_temp$noise_rf)[["estimate"]],2), sep=""))
# g_rf_mu0<- ggplot(data =drf_temp, aes(x= mu0, y = mu0_rf)) + 
#   geom_point()+
#   geom_smooth(method = "lm") + 
#   # coord_cartesian(xlim = c(-8,0), ylim = c(-8,0)) +   
#   theme_Publication() + 
#   theme(axis.text.x = element_blank(),
#         axis.ticks.x = element_blank(),
#         panel.grid.major  = element_blank(),
#         legend.position = "none") +   ylab("refitted mu0") + xlab("mu0") + 
#   labs(title=paste("corr = ", round(cor.test(drf_temp$mu0,drf_temp$mu0_rf)[["estimate"]],2), sep=""))
# 
# g_rf_loggam<- ggplot(data =drf_temp, aes(x= loggam, y = loggam_rf)) + 
#   geom_point()+
#   geom_smooth(method = "lm") + 
#   # coord_cartesian(xlim = c(-8,0), ylim = c(-8,0)) +   
#   theme_Publication() + 
#   theme(axis.text.x = element_blank(),
#         axis.ticks.x = element_blank(),
#         panel.grid.major  = element_blank(),
#         legend.position = "none") +   ylab("refitted log(gamma)") + xlab("log(gamma)") + 
#   labs(title=paste("corr = ", round(cor.test(drf_temp$loggam,drf_temp$loggam_rf)[["estimate"]],2), sep=""))
# plot_grid(g_rf_om_mean, g_rf_noise, g_rf_mu0,g_rf_loggam, ncol = 4)
```



#  model posterior predictive checks 
Plot model predictions for investment and change
```{r, fig.width = 10, fig.height = 9, echo=FALSE, warning= FALSE, eval= FALSE}
# # collect the model predictions  -----------------------------------------------------------
# Pars_extract_set <- rstan::extract(fit_model_gen, pars=c('y_pred',
#                                                          'mu_good_vect',
#                                                          'mu_bad_vect',
#                                                          'mu_good2_vect',
#                                                          'mu_bad2_vect',
#                                                          'pi_good_vect',
#                                                          'pi_bad_vect',
#                                                          'pi_good2_vect',
#                                                          'pi_bad2_vect',
#                                                          'om_good'))
# 
# if (file.exists("Data_beh_with_predictions.rds")) {
#   data_plot_model_pred<-readRDS("Data4PlottingPosteriorChecks.rds")
#   data_plot_model_pred_gen<-readRDS("Data4PlottingPosteriorChecks_genotype.rds")
#   lr1_mat<- readRDS("Data4PlottingLearningRates.rds")
#   y_pred_change <- readRDS("y_pred_change.rds")
#   data_group <- readRDS( "Data_group_with_predictions.rds")
#   data_beh <- readRDS("Data_beh_with_predictions.rds")
# } else  {
#   
#   # 
#   # fit_model_gen <- readRDS("~/mnt/p/userdata/mikusn22/data/Sulpride_trustgame/Model_results/M_gen_lkj_tight_sig_pr.
#   
#   colSdColMeans <- function(x, na.rm=TRUE) {
#     if (na.rm) {
#       n <- colSums(!is.na(x)) # thanks @flodel
#     } else {
#       n <- nrow(x)
#     }
#     colVar <- colMeans(x*x, na.rm=na.rm) - (colMeans(x, na.rm=na.rm))^2
#     return(sqrt(colVar * n/(n-1)))
#   }
#   
#   
#   
#   sample_no <- dim(Pars_extract_set$y_pred)[1]
#   # someData <- rep(sample_no*76*50);
#   lr1_mat<- array(NA, c( sample_no,76*50))
#   y_pred_change<- array(NA, c( sample_no,76*50))
#   y_pred_abschange<- array(NA, c( sample_no,76*50))
#   y_pred_pos<- array(NA, c( sample_no,76*50))
#   y_pred_neg<- array(NA, c( sample_no,76*50))
#   y_pred_incon<- array(NA, c( sample_no,76*50))
#   
#   y_pred_change_mean <- array(NA, c( sample_no,76))
#   y_pred_abschange_mean <- array(NA, c( sample_no,76))
#   y_pred_pos_sum<- array(NA, c( sample_no,76))
#   y_pred_neg_sum<- array(NA, c( sample_no,76))
#   y_pred_incon_sum<- array(NA, c( sample_no,76))
#   y_pred_rec_sum <- array(NA, c( sample_no,76))
#   
#   for (i in 1:sample_no) {
#     if (i/(sample_no/100) == i%/%(sample_no/100) ) print(paste(i/sample_no*100, "%"))
#     
#     y_pred_chain <- Pars_extract_set$y_pred[i,,]
#     y_pred_chain_long <- as.vector(t(y_pred_chain))
#     
#     Inv_Mat_good <- matrix(y_pred_chain_long[data_beh$Trustee == "Good"],nrow = 25)
#     Inv_Mat_bad <- matrix(y_pred_chain_long[data_beh$Trustee == "Bad"],nrow = 25)
#     
#     # dim(Inv_Mat_good)
#     Inv_Mat_good_change <- Inv_Mat_good[2:25,] - Inv_Mat_good[1:24,]
#     Inv_Mat_bad_change <-  Inv_Mat_bad[2:25,] - Inv_Mat_bad[1:24,]
#     # Inv_Mat_bad_change <- c(Na, Inv_Mat_bad_change)
#     # head(Inv_Mat_bad_change_temp)
#     
#     Inv_Mat_good_change_temp <- matrix(0, 25,76)
#     Inv_Mat_good_change_temp[25,] <- NA
#     Inv_Mat_good_change_temp[1:24,] <- Inv_Mat_good_change
#     
#     Inv_Mat_bad_change_temp <- matrix(0, 25,76)
#     Inv_Mat_bad_change_temp[25,] <- NA
#     Inv_Mat_bad_change_temp[1:24,] <- Inv_Mat_bad_change
#     
#     Inv_Mat_good_change_long <- as.numeric(as.vector(Inv_Mat_good_change_temp))
#     Inv_Mat_bad_change_long <-  as.numeric(as.vector(Inv_Mat_bad_change_temp))
#     
#     Change_check <- rep(-99, 3800)
#     Change_check[data_beh$Trustee=="Good"] <- Inv_Mat_good_change_long
#     Change_check[data_beh$Trustee=="Bad"] <- Inv_Mat_bad_change_long
#     
#     mu_good2_vect_chain_long <- as.vector(t(Pars_extract_set$mu_good2_vect[i,,]))
#     mu_good1_vect_chain_long <- 1/(1+ exp(-mu_good2_vect_chain_long));
#     mu_vect_mat_good <-  matrix(mu_good1_vect_chain_long[data_beh$Trustee == "Good"],nrow = 25)
#     
#     dasgmmu2 = as.numeric(data_beh$Backtransfer==1) - mu_good1_vect_chain_long;
#     dasgmmu2_mat <-  matrix(dasgmmu2[data_beh$Trustee == "Good"],nrow = 25)
#     
#     diff_sgmmu2_mat = mu_vect_mat_good[2:25,] - mu_vect_mat_good[1:24,]
#     
#     lr1_good    = dasgmmu2_mat
#     lr1_good[2:25,] = diff_sgmmu2_mat/dasgmmu2_mat[1:24,];
#     lr1_good[1,] = NA
#     lr1_good[dasgmmu2_mat==0] = 0;
#     
#     mu_bad2_vect_chain_long <- as.vector(t(Pars_extract_set$mu_bad2_vect[i,,]))
#     mu_bad1_vect_chain_long <- 1/(1+ exp(-mu_bad2_vect_chain_long));
#     mu_vect_mat_bad <-  matrix(mu_bad1_vect_chain_long[data_beh$Trustee == "Bad"],nrow = 25)
#     
#     dasgmmu2 = as.numeric(data_beh$Backtransfer==1) - mu_bad1_vect_chain_long;
#     dasgmmu2_mat <-  matrix(dasgmmu2[data_beh$Trustee == "Bad"],nrow = 25)
#     
#     diff_sgmmu2_mat = mu_vect_mat_bad[2:25,] - mu_vect_mat_bad[1:24,]
#     
#     lr1_bad    = dasgmmu2_mat
#     lr1_bad[2:25,] = diff_sgmmu2_mat/dasgmmu2_mat[1:24,];
#     lr1_bad[1,] = NA
#     lr1_bad[dasgmmu2_mat==0] = 0;
#     
#     lr1_mat[i,data_beh$Trustee=="Good"] <- as.vector(lr1_good)
#     lr1_mat[i,data_beh$Trustee=="Bad"] <- as.vector(lr1_bad)
#     
#     y_pred_change[i,] <- Change_check
#     y_pred_abschange[i,] <- abs(Change_check)
#     y_pred_pos[i,] <- (data_beh$Backtransfer == 1 & Change_check > 0) | (data_beh$Backtransfer == 1 & y_pred_chain_long == 10)
#     y_pred_neg[i,] <- (data_beh$Backtransfer == -1 & Change_check < 0 ) | (data_beh$Backtransfer == -1 & y_pred_chain_long == 0)
#     y_pred_incon[i,] <- (data_beh$Backtransfer == 1 & Change_check < 0) | (data_beh$Backtransfer == -1 & Change_check > 0)
#     
#     data_pred_temp <- tibble(y_pred_pos_temp = y_pred_pos[i,],
#                              y_pred_neg_temp = y_pred_neg[i,],
#                              y_pred_incon_temp = y_pred_incon[i,],
#                              ID = data_beh$ID,
#                              Change_temp = Change_check) %>% 
#       filter(!is.na(Change_temp)) %>%
#       group_by(ID) %>% 
#       summarise(sumincon = sum(y_pred_incon_temp),
#                 sumposrec = sum(y_pred_pos_temp),
#                 sumnegrec = sum(y_pred_neg_temp),
#                 sumrec = sum(y_pred_pos_temp)+sum(y_pred_neg_temp),
#                 abschange_mean = mean(abs(Change_temp)))
#     
#     # data_pred_temp 
#     y_pred_abschange_mean[i,] <- data_pred_temp$abschange_mean 
#     y_pred_pos_sum[i,]<- data_pred_temp$sumposrec 
#     y_pred_neg_sum[i,]<- data_pred_temp$sumnegrec 
#     y_pred_incon_sum[i,] <-data_pred_temp$sumincon 
#     y_pred_rec_sum[i,] <- data_pred_temp$sumrec 
#     
#     
#   }
#   
#   data_group$pred_mean_pos <- colMeans(y_pred_pos_sum)
#   data_group$pred_mean_neg <- colMeans(y_pred_neg_sum)
#   data_group$pred_mean_rec <- colMeans(y_pred_rec_sum)
#   data_group$pred_mean_incon <- colMeans(y_pred_incon_sum)
#   data_group$pred_mean_abschange <- colMeans(y_pred_abschange_mean)
#   
#   data_group$pred_sd_pos <- colSdColMeans(y_pred_pos_sum)
#   data_group$pred_sd_neg <- colSdColMeans(y_pred_neg_sum)
#   data_group$pred_sd_rec <- colSdColMeans(y_pred_rec_sum)
#   data_group$pred_sd_incon <- colSdColMeans(y_pred_incon_sum)
#   data_group$pred_sd_abschange <- colSdColMeans(y_pred_abschange_mean)
#   
#   
#   # y_pred_change_mean <- colMeans(y_pred_change_mean)
#   # y_pred_change_mean <- colSdColMeans(y_pred_change_mean)
#   
#   # y_pred_change_temp <- y_pred_change[1:10,]
#   # dim(y_pred_change_mean > 0)
#   
#   data_beh$lr1 = colMeans(lr1_mat)
#   
#   data_plot_model_pred<- data_group %>%
#     group_by(drug) %>%
#     summarise(N = n(), 
#               mean_incon= mean(pred_mean_incon),
#               se_incon = sd(pred_mean_incon)/sqrt(N),
#               mean_posrec = mean(pred_mean_pos),
#               se_posrec = sd(pred_mean_pos)/sqrt(N),
#               mean_negrec = mean(pred_mean_neg),
#               se_negrec = sd(pred_mean_neg)/sqrt(N),
#               mean_rec = mean(pred_mean_rec),
#               se_rec = sd(pred_mean_rec)/sqrt(N))
#   
#   data_plot_model_pred_gen<- data_group %>%
#     group_by(drug,ankk) %>%
#     summarise(N = n(), 
#               mean_incon= mean(pred_mean_incon),
#               se_incon = sd(pred_mean_incon)/sqrt(N),
#               mean_posrec = mean(pred_mean_pos),
#               se_posrec = sd(pred_mean_pos)/sqrt(N),
#               mean_negrec = mean(pred_mean_neg),
#               se_negrec = sd(pred_mean_neg)/sqrt(N),
#               mean_rec = mean(pred_mean_rec),
#               se_rec = sd(pred_mean_rec)/sqrt(N))
#   
# 
#   
#   data_beh$predictions <- as.vector(t(colMeans(Pars_extract_set$y_pred)))
#   # data_beh$predictions_sample <- y_pred_random_sample_long
#   # data_beh$predictions_change <- y_pred_abschange_mean
#   data_beh$predictions_abschange <- colMeans(y_pred_abschange)
#   data_beh$mu_good2 <- as.vector(t(colMeans(Pars_extract_set$mu_good2_vect)))
#   data_beh$mu_bad2 <- as.vector(t(colMeans(Pars_extract_set$mu_bad2_vect)))
#   data_beh$pi_good <- as.vector(t(colMeans(Pars_extract_set$pi_good_vect)))
#   data_beh$pi_bad <- as.vector(t(colMeans(Pars_extract_set$pi_bad_vect)))
#   data_beh$pi_good2 <- as.vector(t(colMeans(Pars_extract_set$pi_good2_vect)))
#   data_beh$pi_bad2 <- as.vector(t(colMeans(Pars_extract_set$pi_bad2_vect)))
#   
#   
#   saveRDS(data_plot_model_pred, "Data4PlottingPosteriorChecks.rds")
#   saveRDS(data_plot_model_pred_gen, "Data4PlottingPosteriorChecks_genotype.rds")
#   saveRDS(lr1_mat, "Data4PlottingLearningRates.rds")
#  
#   saveRDS(data_group, "Data_group_with_predictions.rds")
#   saveRDS(data_beh, "Data_beh_with_predictions.rds")
# }
# 
# 
# 
# 
# 
# 
# data_good <- data_beh[data_beh$Trustee =="Good",]
# data_bad <- data_beh[data_beh$Trustee =="Bad",]
# 
# 
# 
# # plot model predictions  across time -------------------------------------------------
# 
# g_investment_smooth_pred <- ggplot() +
#   stat_smooth(data = data_good, aes(x = Trial, y = predictions, group = Treatment, colour = Treatment)) +
#   stat_summary(data = data_good, aes(x = Trial, y = Investment, group = Treatment, colour = Treatment), 
#                geom = "point", fun.y = mean, shape = 17, size = 3)  +
#   stat_smooth(data = data_bad, aes(x = Trial, y = predictions, group = Treatment, colour = Treatment)) +
#   stat_summary(data = data_bad, aes(x = Trial, y = Investment, group = Treatment, colour = Treatment), 
#                geom = "point", fun.y = mean, shape = 17, size = 3)  + 
#   theme_Publication() +
#   
#   theme(axis.ticks.x = element_blank(),
#         legend.position = "none",
#         panel.grid.major  = element_blank()) +
#   ylab("Investment") +  xlab("Trials")+ discrete_scale("colour","Publication",manual_pal(values = c("#386cb0","#fdb462")))
# 
# g_legend_pred <- get_legend(g_investment_smooth_pred + 
#                               theme(legend.position = "right",
#                                     legend.key = element_rect(colour = NA),
#                                     # legend.direction = "horizontal",
#                                     legend.key.size= unit(0.2, "cm"),
#                                     # legend.margin = unit(0, "cm"),
#                                     legend.title = element_text(face="italic")))
# 
# 
# 
# 
# 
# p_change_pred <-data_beh %>% filter(!is.na(abs_change)) %>%
#   ggplot(aes(x = Trial, y = predictions_abschange, group = ID, colour = Treatment)) +#
#   # stat_smooth(aes(group = Treatment)) + 
#   # stat_summary(aes(group = Treatment), geom = "point", fun.y = mean, shape = 17, size = 3) +
#   theme_Publication() +theme(legend.position = "none",
#                              axis.ticks.x = element_blank(),
#                              panel.grid.major  = element_blank()) +
#   ylab("Absolute Investment Change")  + xlab("Trials") +  scale_colour_Publication() +
#   stat_smooth(data = data_beh,aes(x = Trial, y = predictions_abschange,group = Treatment))
# 
# # plot_grid(g_investment_smooth, g_abs_change_time)
# 
# 
# plot_grid(g_investment_smooth_pred, p_change_pred, g_legend_pred, ncol = 3, rel_widths = c(1,1, .4))
# 

```

